{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#import Keras library\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, Input, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.metrics import categorical_accuracy\n",
    "\n",
    "#import spacy, and spacy french model\n",
    "# spacy is used to work on text\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import other libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import collections\n",
    "from six.moves import cPickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/media/dheeraj/9A26F0CB26F0AA01/WORK/IIITD/NLP/Allrecipes/Allrecipes/African/African Deltas'# data directory containing raw texts\n",
    "save_dir = 'save' # directory to store trained NN models\n",
    "\n",
    "#print(os.listdir(train_dir))\n",
    "for img in os.listdir(train_dir):\n",
    "    path = os.path.join(train_dir,img)\n",
    "    img = cv2.imread(path,cv2.IMREAD_COLOR)\n",
    "    #img_array=cv2.imread(img)  #converting the photo to greyscale\n",
    "    #plt.imshow(img)\n",
    "    #plt.show()\n",
    "    #break\n",
    "    #break    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define parameters used in the tutorial\n",
    "data_dir = '/media/dheeraj/9A26F0CB26F0AA01/WORK/IIITD/NLP/Allrecipes/Allrecipes/African/African Deltas'# data directory containing raw texts\n",
    "save_dir = '/media/dheeraj/9A26F0CB26F0AA01/WORK/IIITD/NLP/Allrecipes/Allrecipes/African/save' # directory to store trained NN models\n",
    "\n",
    "\n",
    "vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")\n",
    "sequences_step = 1 #step to create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordlist(doc):\n",
    "    wl = []\n",
    "    for word in doc:\n",
    "        if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
    "            wl.append(word.text.lower())\n",
    "    return wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/dheeraj/9A26F0CB26F0AA01/WORK/IIITD/NLP/Allrecipes/Allrecipes/African/African Deltas/Best Hot Sauce.txt\n",
      "/media/dheeraj/9A26F0CB26F0AA01/WORK/IIITD/NLP/Allrecipes/Allrecipes/African/African Deltas/Chin Chin.txt\n",
      "/media/dheeraj/9A26F0CB26F0AA01/WORK/IIITD/NLP/Allrecipes/Allrecipes/African/African Deltas/Guinean Okra Sauce.txt\n",
      "/media/dheeraj/9A26F0CB26F0AA01/WORK/IIITD/NLP/Allrecipes/Allrecipes/African/African Deltas/Malian Ginger Juice.txt\n",
      "/media/dheeraj/9A26F0CB26F0AA01/WORK/IIITD/NLP/Allrecipes/Allrecipes/African/African Deltas/Shuku Shuku.txt\n",
      "/media/dheeraj/9A26F0CB26F0AA01/WORK/IIITD/NLP/Allrecipes/Allrecipes/African/African Deltas/West African Lime Cake.txt\n"
     ]
    }
   ],
   "source": [
    "wordlist = []\n",
    "\n",
    "for file_name in os.listdir(data_dir):\n",
    "    input_file = os.path.join(data_dir, file_name)\n",
    "    #read data\n",
    "    print(input_file)\n",
    "    with codecs.open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "        \n",
    "    #create sentences\n",
    "    doc = nlp(data)\n",
    "    wl = create_wordlist(doc)\n",
    "    wordlist = wordlist + wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  480\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'save/words_vocab.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f935dc5dd986>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#save the words and vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary_inv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'save/words_vocab.pkl'"
     ]
    }
   ],
   "source": [
    "# count the number of words\n",
    "word_counts = collections.Counter(wordlist)\n",
    "\n",
    "# Mapping from index to word : that's the vocabulary\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "\n",
    "# Mapping from word to index\n",
    "vocab = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "words = [x[0] for x in word_counts.most_common()]\n",
    "\n",
    "#size of the vocabulary\n",
    "vocab_size = len(words)\n",
    "print(\"vocab size: \", vocab_size)\n",
    "\n",
    "#save the words and vocabulary\n",
    "with open(os.path.join(vocab_file), 'wb') as f:\n",
    "    cPickle.dump((words, vocab, vocabulary_inv), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
