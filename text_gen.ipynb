{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#import Keras library\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, Input, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.metrics import categorical_accuracy\n",
    "\n",
    "#import spacy, and spacy french model\n",
    "# spacy is used to work on text\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import other libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import collections\n",
    "from six.moves import cPickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define parameters used in the tutorial\n",
    "data_dir = '/media/dheeraj/9A26F0CB26F0AA01/WORK/IIITD/NLP/Allrecipes/Allrecipes/African/African Deltas'# data directory containing raw texts\n",
    "save_dir = '/media/dheeraj/9A26F0CB26F0AA01/WORK/IIITD/NLP/Allrecipes/Allrecipes/African/African Deltas/save' # directory to store trained NN models\n",
    "\n",
    "\n",
    "vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")\n",
    "sequences_step = 1 #step to create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordlist(doc):\n",
    "    wl = []\n",
    "    for word in doc:\n",
    "        if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
    "            wl.append(word.text.lower())\n",
    "    return wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/dheeraj/9A26F0CB26F0AA01/WORK/IIITD/NLP/Allrecipes/Allrecipes/African/African Deltas/Best Hot Sauce.txt\n",
      "/media/dheeraj/9A26F0CB26F0AA01/WORK/IIITD/NLP/Allrecipes/Allrecipes/African/African Deltas/Chin Chin.txt\n",
      "/media/dheeraj/9A26F0CB26F0AA01/WORK/IIITD/NLP/Allrecipes/Allrecipes/African/African Deltas/Guinean Okra Sauce.txt\n",
      "/media/dheeraj/9A26F0CB26F0AA01/WORK/IIITD/NLP/Allrecipes/Allrecipes/African/African Deltas/Malian Ginger Juice.txt\n",
      "/media/dheeraj/9A26F0CB26F0AA01/WORK/IIITD/NLP/Allrecipes/Allrecipes/African/African Deltas/save\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/media/dheeraj/9A26F0CB26F0AA01/WORK/IIITD/NLP/Allrecipes/Allrecipes/African/African Deltas/save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-ea9635c7d72e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#read data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/codecs.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, encoding, errors, buffering)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;31m# Force opening of the file in binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/media/dheeraj/9A26F0CB26F0AA01/WORK/IIITD/NLP/Allrecipes/Allrecipes/African/African Deltas/save'"
     ]
    }
   ],
   "source": [
    "wordlist = []\n",
    "\n",
    "for file_name in os.listdir(data_dir):\n",
    "    input_file = os.path.join(data_dir, file_name)\n",
    "    #read data\n",
    "    print(input_file)\n",
    "    with codecs.open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "        \n",
    "    #create sentences\n",
    "    doc = nlp(data)\n",
    "    wl = create_wordlist(doc)\n",
    "    wordlist = wordlist + wl\n",
    "    \n",
    "    \n",
    "print(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  480\n"
     ]
    }
   ],
   "source": [
    "# count the number of words\n",
    "word_counts = collections.Counter(wordlist)\n",
    "\n",
    "# Mapping from index to word : that's the vocabulary\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "\n",
    "# Mapping from word to index\n",
    "vocab = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "words = [x[0] for x in word_counts.most_common()]\n",
    "\n",
    "#size of the vocabulary\n",
    "vocab_size = len(words)\n",
    "print(\"vocab size: \", vocab_size)\n",
    "\n",
    "#save the words and vocabulary\n",
    "with open(os.path.join(vocab_file), 'wb') as f:\n",
    "    cPickle.dump((words, vocab, vocabulary_inv), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 1664\n"
     ]
    }
   ],
   "source": [
    "#create sequences\n",
    "sequences = []\n",
    "next_words = []\n",
    "seq_length=30\n",
    "for i in range(0, len(wordlist) - seq_length, sequences_step):\n",
    "    sequences.append(wordlist[i: i + seq_length])\n",
    "    next_words.append(wordlist[i + seq_length])\n",
    "\n",
    "print('nb sequences:', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1664, 30, 480)\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((len(sequences), seq_length, vocab_size), dtype=np.bool)\n",
    "print(X.shape)\n",
    "y = np.zeros((len(sequences), vocab_size), dtype=np.bool)\n",
    "for i, sentence in enumerate(sequences):\n",
    "    for t, word in enumerate(sentence):\n",
    "        X[i, t, vocab[word]] = 1\n",
    "    y[i, vocab[next_words[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_lstm_model(seq_length, vocab_size):\n",
    "    print('Build LSTM model.')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\"),input_shape=(seq_length, vocab_size)))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    callbacks=[EarlyStopping(patience=2, monitor='val_loss')]\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[categorical_accuracy])\n",
    "    print(\"model built!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM model.\n",
      "model built!\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 512)               1509376   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 480)               246240    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 480)               0         \n",
      "=================================================================\n",
      "Total params: 1,755,616\n",
      "Trainable params: 1,755,616\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_size = 256 # size of RNN\n",
    "seq_length = 30 # sequence length\n",
    "learning_rate = 0.001 #learning rate\n",
    "\n",
    "md = bidirectional_lstm_model(seq_length, vocab_size)\n",
    "md.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1497 samples, validate on 167 samples\n",
      "Epoch 1/50\n",
      "1497/1497 [==============================] - 36s 24ms/step - loss: 4.2468 - categorical_accuracy: 0.1242 - val_loss: 6.0969 - val_categorical_accuracy: 0.1018\n",
      "Epoch 2/50\n",
      "1497/1497 [==============================] - 38s 25ms/step - loss: 4.1290 - categorical_accuracy: 0.1530 - val_loss: 6.0906 - val_categorical_accuracy: 0.0838\n",
      "Epoch 3/50\n",
      "1497/1497 [==============================] - 39s 26ms/step - loss: 3.9725 - categorical_accuracy: 0.1717 - val_loss: 6.1453 - val_categorical_accuracy: 0.0958\n",
      "Epoch 4/50\n",
      "1497/1497 [==============================] - 41s 28ms/step - loss: 3.7932 - categorical_accuracy: 0.1844 - val_loss: 5.9997 - val_categorical_accuracy: 0.1078\n",
      "Epoch 5/50\n",
      "1497/1497 [==============================] - 48s 32ms/step - loss: 3.6716 - categorical_accuracy: 0.2104 - val_loss: 6.4031 - val_categorical_accuracy: 0.0898\n",
      "Epoch 6/50\n",
      "1497/1497 [==============================] - 47s 32ms/step - loss: 3.4910 - categorical_accuracy: 0.2358 - val_loss: 6.5823 - val_categorical_accuracy: 0.0838\n",
      "Epoch 7/50\n",
      "1497/1497 [==============================] - 56s 38ms/step - loss: 3.4430 - categorical_accuracy: 0.2465 - val_loss: 6.1760 - val_categorical_accuracy: 0.1317\n",
      "Epoch 8/50\n",
      "1497/1497 [==============================] - 49s 33ms/step - loss: 3.2189 - categorical_accuracy: 0.2846 - val_loss: 6.3172 - val_categorical_accuracy: 0.1257\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # minibatch size\n",
    "num_epochs = 50 # number of epochs\n",
    "\n",
    "checkpoints=[EarlyStopping(patience=4, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath=save_dir + \"/\" + 'my_model_gen_sentences.{epoch:02d}-{val_loss:.2f}.hdf5',\\\n",
    "                           monitor='val_loss', verbose=0, mode='auto', period=2)]\n",
    "#fit the model\n",
    "history = md.fit(X, y,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True,\n",
    "                 epochs=num_epochs,\n",
    "                 validation_split=0.1, \n",
    "                 callbacks=checkpoints)\n",
    "                 \n",
    "#save the model\n",
    "md.save(save_dir + \"/\" + 'my_model_generate_sentences.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
