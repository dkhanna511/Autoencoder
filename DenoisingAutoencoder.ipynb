{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-c7ff5b64a861>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/dheeraj/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/dheeraj/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/dheeraj/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/dheeraj/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist=input_data.read_data_sets(\"MNIST_data/\")\n",
    "trX, trY, teX, teY= mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingAutoEncoder(object):\n",
    "    def __init__(self, m, n, eta=0.01):\n",
    "        #m : number of neuons in input/output layer\n",
    "        # n : number of neurons in hidden layer\n",
    "        \n",
    "        self._m= m\n",
    "        self._n= n\n",
    "        self.learning_rate=eta\n",
    "        \n",
    "        #Create a Computational graph\n",
    "        \n",
    "        #weights and biases\n",
    "        self._w1=tf.Variable(tf.random_normal(shape=(self._m, self._n)))\n",
    "        self._w2=tf.Variable(tf.random.normal(shape=(self._n, self._m)))\n",
    "        \n",
    "        self._b1=tf.Variable(np.zeros(self._n).astype(np.float32))\n",
    "        self._b2=tf.Variable(np.zeros(self._m).astype(np.float32))\n",
    "        \n",
    "        #Placeholder for inputs\n",
    "        \n",
    "        self._X=tf.placeholder(tf.float32, shape=[None, self._m])\n",
    "        \n",
    "        self._X_noisy= tf.placeholder(tf.float32, shape=[None, self._m])\n",
    "\n",
    "        self.y=self.encoder(self._X)\n",
    "        self.r=self.decoder(self.y)\n",
    "        \n",
    "        error=self.r= self._X\n",
    "        \n",
    "        self._loss= tf.reduce_mean(tf.pow(error, 2))\n",
    "        \n",
    "        alpha=0.05\n",
    "        kl_div_loss=tf.reduce_mean(self.kl_div(0.02, tf.reduce_mean(self.y, 0)))\n",
    "        loss=self._loss + alpha*kl_div_loss\n",
    "        \n",
    "        self._opt= tf.train.AdamOptimizer(self.learning_rate).minimize(loss)\n",
    "        \n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h=tf.matmul(x, self._w1) + self._b1\n",
    "        return tf.nn.sigmoid(h)\n",
    "    \n",
    "    def decoder(self, x):\n",
    "        h=tf.matmul(x, self._w2) + self._b2\n",
    "        return tf.nn.sigmoid(h)\n",
    "    \n",
    "    def set_session(self, session):\n",
    "        self.session = session\n",
    "        \n",
    "    def reconstruct(self, x):\n",
    "        h= self.encoder(x)\n",
    "        r=self.decoder(h)\n",
    "        return self.session.run(r, feed_dict= {self._X : x})\n",
    "    \n",
    "    def kl_div(self, rho, rho_hat):\n",
    "        term2_num = tf.constant(1.) - rho\n",
    "        term2_den = tf.constant(1.) - rho_hat\n",
    "        \n",
    "        kl= self.logfunc(rho, rho_hat) + self.logfunc(term2_num, term2_den)\n",
    "        return kl\n",
    "    \n",
    "    def logfunc(self, x1, x2):\n",
    "        return tf.multiply(x1, tf.log(tf.div(x1, x2)))\n",
    "    \n",
    "    def corrupt(self, x):\n",
    "        return x*tf.cast(tf.random_uniform(shape=tf.shape(x), minval=0, maxval=2), tf.float32)\n",
    "    \n",
    "    def getWeights(self):\n",
    "        return self.session.run([self._w1, self._w2, self._b1, self._b2])\n",
    "    \n",
    "    def fit(self, X, Xorg, epochs=50, batch_size= 100):\n",
    "        N, D= X.shape\n",
    "        num_batches= N//batch_size\n",
    "        \n",
    "        obj=[]\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            for j in range(num_batches):\n",
    "                batch= X[j*batch_size : (j* batch_size + batch_size)]\n",
    "                batchO= Xorg[j* batch_size : (j *batch_size + batch_size)]\n",
    "                \n",
    "                _, ob= self.session.run([self._opt, self._loss], feed_dict= {self._X:batchO, self._X_noisy: batch})\n",
    "                \n",
    "                if j%100 ==0:\n",
    "                    print('training epoch {0} batch {2} cost {1}'.format(i, ob, j))\n",
    "                    obj.append(ob)\n",
    "        return obj\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corruption( x, noise_factor = 0.3):   # corruption of the input\n",
    "    noisy_imgs= x+ noise_factor*np.random.randn(*x.shape)\n",
    "    noisy_imgs=np.clip(noisy_imgs, 0. , 1.)\n",
    "    return noisy_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 0 batch 0 cost 0.11744999885559082\n",
      "training epoch 0 batch 100 cost 0.10667049139738083\n",
      "training epoch 0 batch 200 cost 0.10857931524515152\n",
      "training epoch 0 batch 300 cost 0.11918950080871582\n",
      "training epoch 0 batch 400 cost 0.1134793758392334\n",
      "training epoch 0 batch 500 cost 0.11265762150287628\n",
      "training epoch 1 batch 0 cost 0.11744999885559082\n",
      "training epoch 1 batch 100 cost 0.10667049139738083\n",
      "training epoch 1 batch 200 cost 0.10857931524515152\n",
      "training epoch 1 batch 300 cost 0.11918950080871582\n",
      "training epoch 1 batch 400 cost 0.1134793758392334\n",
      "training epoch 1 batch 500 cost 0.11265762150287628\n",
      "training epoch 2 batch 0 cost 0.11744999885559082\n",
      "training epoch 2 batch 100 cost 0.10667049139738083\n",
      "training epoch 2 batch 200 cost 0.10857931524515152\n",
      "training epoch 2 batch 300 cost 0.11918950080871582\n",
      "training epoch 2 batch 400 cost 0.1134793758392334\n",
      "training epoch 2 batch 500 cost 0.11265762150287628\n",
      "training epoch 3 batch 0 cost 0.11744999885559082\n",
      "training epoch 3 batch 100 cost 0.10667049139738083\n",
      "training epoch 3 batch 200 cost 0.10857931524515152\n",
      "training epoch 3 batch 300 cost 0.11918950080871582\n",
      "training epoch 3 batch 400 cost 0.1134793758392334\n",
      "training epoch 3 batch 500 cost 0.11265762150287628\n",
      "training epoch 4 batch 0 cost 0.11744999885559082\n",
      "training epoch 4 batch 100 cost 0.10667049139738083\n",
      "training epoch 4 batch 200 cost 0.10857931524515152\n",
      "training epoch 4 batch 300 cost 0.11918950080871582\n",
      "training epoch 4 batch 400 cost 0.1134793758392334\n",
      "training epoch 4 batch 500 cost 0.11265762150287628\n",
      "training epoch 5 batch 0 cost 0.11744999885559082\n",
      "training epoch 5 batch 100 cost 0.10667049139738083\n",
      "training epoch 5 batch 200 cost 0.10857931524515152\n",
      "training epoch 5 batch 300 cost 0.11918950080871582\n",
      "training epoch 5 batch 400 cost 0.1134793758392334\n",
      "training epoch 5 batch 500 cost 0.11265762150287628\n",
      "training epoch 6 batch 0 cost 0.11744999885559082\n",
      "training epoch 6 batch 100 cost 0.10667049139738083\n",
      "training epoch 6 batch 200 cost 0.10857931524515152\n",
      "training epoch 6 batch 300 cost 0.11918950080871582\n",
      "training epoch 6 batch 400 cost 0.1134793758392334\n",
      "training epoch 6 batch 500 cost 0.11265762150287628\n",
      "training epoch 7 batch 0 cost 0.11744999885559082\n",
      "training epoch 7 batch 100 cost 0.10667049139738083\n",
      "training epoch 7 batch 200 cost 0.10857931524515152\n",
      "training epoch 7 batch 300 cost 0.11918950080871582\n",
      "training epoch 7 batch 400 cost 0.1134793758392334\n",
      "training epoch 7 batch 500 cost 0.11265762150287628\n",
      "training epoch 8 batch 0 cost 0.11744999885559082\n",
      "training epoch 8 batch 100 cost 0.10667049139738083\n",
      "training epoch 8 batch 200 cost 0.10857931524515152\n",
      "training epoch 8 batch 300 cost 0.11918950080871582\n",
      "training epoch 8 batch 400 cost 0.1134793758392334\n",
      "training epoch 8 batch 500 cost 0.11265762150287628\n",
      "training epoch 9 batch 0 cost 0.11744999885559082\n",
      "training epoch 9 batch 100 cost 0.10667049139738083\n",
      "training epoch 9 batch 200 cost 0.10857931524515152\n",
      "training epoch 9 batch 300 cost 0.11918950080871582\n",
      "training epoch 9 batch 400 cost 0.1134793758392334\n",
      "training epoch 9 batch 500 cost 0.11265762150287628\n",
      "training epoch 10 batch 0 cost 0.11744999885559082\n",
      "training epoch 10 batch 100 cost 0.10667049139738083\n",
      "training epoch 10 batch 200 cost 0.10857931524515152\n",
      "training epoch 10 batch 300 cost 0.11918950080871582\n",
      "training epoch 10 batch 400 cost 0.1134793758392334\n",
      "training epoch 10 batch 500 cost 0.11265762150287628\n",
      "training epoch 11 batch 0 cost 0.11744999885559082\n",
      "training epoch 11 batch 100 cost 0.10667049139738083\n",
      "training epoch 11 batch 200 cost 0.10857931524515152\n",
      "training epoch 11 batch 300 cost 0.11918950080871582\n",
      "training epoch 11 batch 400 cost 0.1134793758392334\n",
      "training epoch 11 batch 500 cost 0.11265762150287628\n",
      "training epoch 12 batch 0 cost 0.11744999885559082\n",
      "training epoch 12 batch 100 cost 0.10667049139738083\n",
      "training epoch 12 batch 200 cost 0.10857931524515152\n",
      "training epoch 12 batch 300 cost 0.11918950080871582\n",
      "training epoch 12 batch 400 cost 0.1134793758392334\n",
      "training epoch 12 batch 500 cost 0.11265762150287628\n",
      "training epoch 13 batch 0 cost 0.11744999885559082\n",
      "training epoch 13 batch 100 cost 0.10667049139738083\n",
      "training epoch 13 batch 200 cost 0.10857931524515152\n",
      "training epoch 13 batch 300 cost 0.11918950080871582\n",
      "training epoch 13 batch 400 cost 0.1134793758392334\n",
      "training epoch 13 batch 500 cost 0.11265762150287628\n",
      "training epoch 14 batch 0 cost 0.11744999885559082\n",
      "training epoch 14 batch 100 cost 0.10667049139738083\n",
      "training epoch 14 batch 200 cost 0.10857931524515152\n",
      "training epoch 14 batch 300 cost 0.11918950080871582\n",
      "training epoch 14 batch 400 cost 0.1134793758392334\n",
      "training epoch 14 batch 500 cost 0.11265762150287628\n",
      "training epoch 15 batch 0 cost 0.11744999885559082\n",
      "training epoch 15 batch 100 cost 0.10667049139738083\n",
      "training epoch 15 batch 200 cost 0.10857931524515152\n",
      "training epoch 15 batch 300 cost 0.11918950080871582\n",
      "training epoch 15 batch 400 cost 0.1134793758392334\n",
      "training epoch 15 batch 500 cost 0.11265762150287628\n",
      "training epoch 16 batch 0 cost 0.11744999885559082\n",
      "training epoch 16 batch 100 cost 0.10667049139738083\n",
      "training epoch 16 batch 200 cost 0.10857931524515152\n",
      "training epoch 16 batch 300 cost 0.11918950080871582\n",
      "training epoch 16 batch 400 cost 0.1134793758392334\n",
      "training epoch 16 batch 500 cost 0.11265762150287628\n",
      "training epoch 17 batch 0 cost 0.11744999885559082\n",
      "training epoch 17 batch 100 cost 0.10667049139738083\n",
      "training epoch 17 batch 200 cost 0.10857931524515152\n",
      "training epoch 17 batch 300 cost 0.11918950080871582\n",
      "training epoch 17 batch 400 cost 0.1134793758392334\n",
      "training epoch 17 batch 500 cost 0.11265762150287628\n",
      "training epoch 18 batch 0 cost 0.11744999885559082\n",
      "training epoch 18 batch 100 cost 0.10667049139738083\n",
      "training epoch 18 batch 200 cost 0.10857931524515152\n",
      "training epoch 18 batch 300 cost 0.11918950080871582\n",
      "training epoch 18 batch 400 cost 0.1134793758392334\n",
      "training epoch 18 batch 500 cost 0.11265762150287628\n",
      "training epoch 19 batch 0 cost 0.11744999885559082\n",
      "training epoch 19 batch 100 cost 0.10667049139738083\n",
      "training epoch 19 batch 200 cost 0.10857931524515152\n",
      "training epoch 19 batch 300 cost 0.11918950080871582\n",
      "training epoch 19 batch 400 cost 0.1134793758392334\n",
      "training epoch 19 batch 500 cost 0.11265762150287628\n",
      "training epoch 20 batch 0 cost 0.11744999885559082\n",
      "training epoch 20 batch 100 cost 0.10667049139738083\n",
      "training epoch 20 batch 200 cost 0.10857931524515152\n",
      "training epoch 20 batch 300 cost 0.11918950080871582\n",
      "training epoch 20 batch 400 cost 0.1134793758392334\n",
      "training epoch 20 batch 500 cost 0.11265762150287628\n",
      "training epoch 21 batch 0 cost 0.11744999885559082\n",
      "training epoch 21 batch 100 cost 0.10667049139738083\n",
      "training epoch 21 batch 200 cost 0.10857931524515152\n",
      "training epoch 21 batch 300 cost 0.11918950080871582\n",
      "training epoch 21 batch 400 cost 0.1134793758392334\n",
      "training epoch 21 batch 500 cost 0.11265762150287628\n",
      "training epoch 22 batch 0 cost 0.11744999885559082\n",
      "training epoch 22 batch 100 cost 0.10667049139738083\n",
      "training epoch 22 batch 200 cost 0.10857931524515152\n",
      "training epoch 22 batch 300 cost 0.11918950080871582\n",
      "training epoch 22 batch 400 cost 0.1134793758392334\n",
      "training epoch 22 batch 500 cost 0.11265762150287628\n",
      "training epoch 23 batch 0 cost 0.11744999885559082\n",
      "training epoch 23 batch 100 cost 0.10667049139738083\n",
      "training epoch 23 batch 200 cost 0.10857931524515152\n",
      "training epoch 23 batch 300 cost 0.11918950080871582\n",
      "training epoch 23 batch 400 cost 0.1134793758392334\n",
      "training epoch 23 batch 500 cost 0.11265762150287628\n",
      "training epoch 24 batch 0 cost 0.11744999885559082\n",
      "training epoch 24 batch 100 cost 0.10667049139738083\n",
      "training epoch 24 batch 200 cost 0.10857931524515152\n",
      "training epoch 24 batch 300 cost 0.11918950080871582\n",
      "training epoch 24 batch 400 cost 0.1134793758392334\n",
      "training epoch 24 batch 500 cost 0.11265762150287628\n",
      "training epoch 25 batch 0 cost 0.11744999885559082\n",
      "training epoch 25 batch 100 cost 0.10667049139738083\n",
      "training epoch 25 batch 200 cost 0.10857931524515152\n",
      "training epoch 25 batch 300 cost 0.11918950080871582\n",
      "training epoch 25 batch 400 cost 0.1134793758392334\n",
      "training epoch 25 batch 500 cost 0.11265762150287628\n",
      "training epoch 26 batch 0 cost 0.11744999885559082\n",
      "training epoch 26 batch 100 cost 0.10667049139738083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 26 batch 200 cost 0.10857931524515152\n",
      "training epoch 26 batch 300 cost 0.11918950080871582\n",
      "training epoch 26 batch 400 cost 0.1134793758392334\n",
      "training epoch 26 batch 500 cost 0.11265762150287628\n",
      "training epoch 27 batch 0 cost 0.11744999885559082\n",
      "training epoch 27 batch 100 cost 0.10667049139738083\n",
      "training epoch 27 batch 200 cost 0.10857931524515152\n",
      "training epoch 27 batch 300 cost 0.11918950080871582\n",
      "training epoch 27 batch 400 cost 0.1134793758392334\n",
      "training epoch 27 batch 500 cost 0.11265762150287628\n",
      "training epoch 28 batch 0 cost 0.11744999885559082\n",
      "training epoch 28 batch 100 cost 0.10667049139738083\n",
      "training epoch 28 batch 200 cost 0.10857931524515152\n",
      "training epoch 28 batch 300 cost 0.11918950080871582\n",
      "training epoch 28 batch 400 cost 0.1134793758392334\n",
      "training epoch 28 batch 500 cost 0.11265762150287628\n",
      "training epoch 29 batch 0 cost 0.11744999885559082\n",
      "training epoch 29 batch 100 cost 0.10667049139738083\n",
      "training epoch 29 batch 200 cost 0.10857931524515152\n",
      "training epoch 29 batch 300 cost 0.11918950080871582\n",
      "training epoch 29 batch 400 cost 0.1134793758392334\n",
      "training epoch 29 batch 500 cost 0.11265762150287628\n",
      "training epoch 30 batch 0 cost 0.11744999885559082\n",
      "training epoch 30 batch 100 cost 0.10667049139738083\n",
      "training epoch 30 batch 200 cost 0.10857931524515152\n",
      "training epoch 30 batch 300 cost 0.11918950080871582\n",
      "training epoch 30 batch 400 cost 0.1134793758392334\n",
      "training epoch 30 batch 500 cost 0.11265762150287628\n",
      "training epoch 31 batch 0 cost 0.11744999885559082\n",
      "training epoch 31 batch 100 cost 0.10667049139738083\n",
      "training epoch 31 batch 200 cost 0.10857931524515152\n",
      "training epoch 31 batch 300 cost 0.11918950080871582\n",
      "training epoch 31 batch 400 cost 0.1134793758392334\n",
      "training epoch 31 batch 500 cost 0.11265762150287628\n",
      "training epoch 32 batch 0 cost 0.11744999885559082\n",
      "training epoch 32 batch 100 cost 0.10667049139738083\n",
      "training epoch 32 batch 200 cost 0.10857931524515152\n",
      "training epoch 32 batch 300 cost 0.11918950080871582\n",
      "training epoch 32 batch 400 cost 0.1134793758392334\n",
      "training epoch 32 batch 500 cost 0.11265762150287628\n",
      "training epoch 33 batch 0 cost 0.11744999885559082\n",
      "training epoch 33 batch 100 cost 0.10667049139738083\n",
      "training epoch 33 batch 200 cost 0.10857931524515152\n",
      "training epoch 33 batch 300 cost 0.11918950080871582\n",
      "training epoch 33 batch 400 cost 0.1134793758392334\n",
      "training epoch 33 batch 500 cost 0.11265762150287628\n",
      "training epoch 34 batch 0 cost 0.11744999885559082\n",
      "training epoch 34 batch 100 cost 0.10667049139738083\n",
      "training epoch 34 batch 200 cost 0.10857931524515152\n",
      "training epoch 34 batch 300 cost 0.11918950080871582\n",
      "training epoch 34 batch 400 cost 0.1134793758392334\n",
      "training epoch 34 batch 500 cost 0.11265762150287628\n",
      "training epoch 35 batch 0 cost 0.11744999885559082\n",
      "training epoch 35 batch 100 cost 0.10667049139738083\n",
      "training epoch 35 batch 200 cost 0.10857931524515152\n",
      "training epoch 35 batch 300 cost 0.11918950080871582\n",
      "training epoch 35 batch 400 cost 0.1134793758392334\n",
      "training epoch 35 batch 500 cost 0.11265762150287628\n",
      "training epoch 36 batch 0 cost 0.11744999885559082\n",
      "training epoch 36 batch 100 cost 0.10667049139738083\n",
      "training epoch 36 batch 200 cost 0.10857931524515152\n",
      "training epoch 36 batch 300 cost 0.11918950080871582\n",
      "training epoch 36 batch 400 cost 0.1134793758392334\n",
      "training epoch 36 batch 500 cost 0.11265762150287628\n",
      "training epoch 37 batch 0 cost 0.11744999885559082\n",
      "training epoch 37 batch 100 cost 0.10667049139738083\n",
      "training epoch 37 batch 200 cost 0.10857931524515152\n",
      "training epoch 37 batch 300 cost 0.11918950080871582\n",
      "training epoch 37 batch 400 cost 0.1134793758392334\n",
      "training epoch 37 batch 500 cost 0.11265762150287628\n",
      "training epoch 38 batch 0 cost 0.11744999885559082\n",
      "training epoch 38 batch 100 cost 0.10667049139738083\n",
      "training epoch 38 batch 200 cost 0.10857931524515152\n",
      "training epoch 38 batch 300 cost 0.11918950080871582\n",
      "training epoch 38 batch 400 cost 0.1134793758392334\n",
      "training epoch 38 batch 500 cost 0.11265762150287628\n",
      "training epoch 39 batch 0 cost 0.11744999885559082\n",
      "training epoch 39 batch 100 cost 0.10667049139738083\n",
      "training epoch 39 batch 200 cost 0.10857931524515152\n",
      "training epoch 39 batch 300 cost 0.11918950080871582\n",
      "training epoch 39 batch 400 cost 0.1134793758392334\n",
      "training epoch 39 batch 500 cost 0.11265762150287628\n",
      "training epoch 40 batch 0 cost 0.11744999885559082\n",
      "training epoch 40 batch 100 cost 0.10667049139738083\n",
      "training epoch 40 batch 200 cost 0.10857931524515152\n",
      "training epoch 40 batch 300 cost 0.11918950080871582\n",
      "training epoch 40 batch 400 cost 0.1134793758392334\n",
      "training epoch 40 batch 500 cost 0.11265762150287628\n",
      "training epoch 41 batch 0 cost 0.11744999885559082\n",
      "training epoch 41 batch 100 cost 0.10667049139738083\n",
      "training epoch 41 batch 200 cost 0.10857931524515152\n",
      "training epoch 41 batch 300 cost 0.11918950080871582\n",
      "training epoch 41 batch 400 cost 0.1134793758392334\n",
      "training epoch 41 batch 500 cost 0.11265762150287628\n",
      "training epoch 42 batch 0 cost 0.11744999885559082\n",
      "training epoch 42 batch 100 cost 0.10667049139738083\n",
      "training epoch 42 batch 200 cost 0.10857931524515152\n",
      "training epoch 42 batch 300 cost 0.11918950080871582\n",
      "training epoch 42 batch 400 cost 0.1134793758392334\n",
      "training epoch 42 batch 500 cost 0.11265762150287628\n",
      "training epoch 43 batch 0 cost 0.11744999885559082\n",
      "training epoch 43 batch 100 cost 0.10667049139738083\n",
      "training epoch 43 batch 200 cost 0.10857931524515152\n",
      "training epoch 43 batch 300 cost 0.11918950080871582\n",
      "training epoch 43 batch 400 cost 0.1134793758392334\n",
      "training epoch 43 batch 500 cost 0.11265762150287628\n",
      "training epoch 44 batch 0 cost 0.11744999885559082\n",
      "training epoch 44 batch 100 cost 0.10667049139738083\n",
      "training epoch 44 batch 200 cost 0.10857931524515152\n",
      "training epoch 44 batch 300 cost 0.11918950080871582\n",
      "training epoch 44 batch 400 cost 0.1134793758392334\n",
      "training epoch 44 batch 500 cost 0.11265762150287628\n",
      "training epoch 45 batch 0 cost 0.11744999885559082\n",
      "training epoch 45 batch 100 cost 0.10667049139738083\n",
      "training epoch 45 batch 200 cost 0.10857931524515152\n",
      "training epoch 45 batch 300 cost 0.11918950080871582\n",
      "training epoch 45 batch 400 cost 0.1134793758392334\n",
      "training epoch 45 batch 500 cost 0.11265762150287628\n",
      "training epoch 46 batch 0 cost 0.11744999885559082\n",
      "training epoch 46 batch 100 cost 0.10667049139738083\n",
      "training epoch 46 batch 200 cost 0.10857931524515152\n",
      "training epoch 46 batch 300 cost 0.11918950080871582\n",
      "training epoch 46 batch 400 cost 0.1134793758392334\n",
      "training epoch 46 batch 500 cost 0.11265762150287628\n",
      "training epoch 47 batch 0 cost 0.11744999885559082\n",
      "training epoch 47 batch 100 cost 0.10667049139738083\n",
      "training epoch 47 batch 200 cost 0.10857931524515152\n",
      "training epoch 47 batch 300 cost 0.11918950080871582\n",
      "training epoch 47 batch 400 cost 0.1134793758392334\n",
      "training epoch 47 batch 500 cost 0.11265762150287628\n",
      "training epoch 48 batch 0 cost 0.11744999885559082\n",
      "training epoch 48 batch 100 cost 0.10667049139738083\n",
      "training epoch 48 batch 200 cost 0.10857931524515152\n",
      "training epoch 48 batch 300 cost 0.11918950080871582\n",
      "training epoch 48 batch 400 cost 0.1134793758392334\n",
      "training epoch 48 batch 500 cost 0.11265762150287628\n",
      "training epoch 49 batch 0 cost 0.11744999885559082\n",
      "training epoch 49 batch 100 cost 0.10667049139738083\n",
      "training epoch 49 batch 200 cost 0.10857931524515152\n",
      "training epoch 49 batch 300 cost 0.11918950080871582\n",
      "training epoch 49 batch 400 cost 0.1134793758392334\n",
      "training epoch 49 batch 500 cost 0.11265762150287628\n"
     ]
    }
   ],
   "source": [
    "n_hidden= 1000\n",
    "Xtrain= trX.astype(np.float32)\n",
    "Xtrain_noisy= corruption (Xtrain).astype(np.float32)\n",
    "Xtest= teX.astype(np.float32)\n",
    "# noise= Xtest * np.random.randint(0, 2, Xtest.shape).astype(np.float32)\n",
    "Xtest_noisy= corruption(Xtest).astype(np.float32)\n",
    "\n",
    "_, m= Xtrain.shape\n",
    "\n",
    "dae= DenoisingAutoEncoder( m, n_hidden)\n",
    "\n",
    "#Initialise all variables\n",
    "init= tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    dae.set_session(sess)\n",
    "    err= dae.fit(Xtrain_noisy, Xtrain, epochs=50)\n",
    "    out= dae.reconstruct(Xtest_noisy[0:100])\n",
    "    #w1, w2, b1, b2 = dae.getWeights()\n",
    "    #red= dae.reduced_dimension(Xtrain)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.plt(err, out)\\nplt.xlabel('epochs')\\nplt.ylabel('reconstruction Loss (MSE)')\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "plt.plt(err, out)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('reconstruction Loss (MSE)')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-9fe5f867ba95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#plotting original and reconstructed image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxarr\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharex\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mXtest_noisy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "#plotting original and reconstructed image\n",
    "row, col= 2,8\n",
    "idx= np.random(0, 100, row*col//2)\n",
    "f, axarr= plt.subplots(row, col, sharex= True, sharey= True, figsize= (20, 4))\n",
    "for fig, row in zip([Xtest_noisy, out], axarr):\n",
    "    for i, ax in zip(idx, row):\n",
    "        ax.imshow(fig[i].reshape((28, 28)))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
